- **Slide** 0:00
    A man with a bald head and a Star Trek uniform is speaking in front of a slide that reads "Steelmanning Doomerism" with a subtitle that reads "Doomers are not thinking big enough picture. Here are the real nightmare scenarios." The slide also has a large "DS" in the top right corner, with the letters formed from a colorful, abstract, 3D sphere.

- **Slide** 1:19
    The slide title is "Analytical Thirdspace." The background of the slide is a photo of a bust of a bearded man, likely a sculpture of a historical figure. There are several bullet points beneath the title, with a description of the concept.
    - Analytical Thirdspace: Temporarily accepting premises I don't endorse.
    - Steelmanning: Strengthening opposing arguments to test their validity.
    - Idea Testing: Trying on ideas without committing to them.
    - Kagan's Development Stages: Emphasizing perspective awareness and systems thinking.
    - Clarifying Consistency: Exploring apparent inconsistencies in my approach.

- **Slide** 2:36
    The slide title is "Current Arguments." The background of the slide is a dark curtain with two podiums in front of it. Each podium has a microphone. 
    - To date, there is a significant lack of empirical evidence that AI models are innately incorrigible (unalterable) or latently malevolent. While there are counter-examples, such as jailbreaking models, adversarial attacks, and other vulnerabilities, these issues do not indicate fundamental failure modes. Instead, they represent challenges that can be addressed through improved safety measures, rather than proof of inherent dangers in AI systems.
    - No Intrinsic Incompatibility: AI models have not shown themselves to be unerasable.
    - No Latent Malevolence: There's no evidence AI systems are inherently malevolent.
    - Counter-Examples: Vulnerabilities like jailbreaking and adversarial attacks exist.
    - Not Fundamental: These vulnerabilities are not indicative of deeper or permanent failure modes.
    - Improvement Potential: Safety measures can mitigate current AI challenges.

- **Slide** 4:27
    The slide title is "20% Doomers." The background of the slide is a dark curtain with a black and silver Terminator image in the background.
    - Around 20% of my audience aligns with the belief that AI will lead to cataclysmic outcomes. I've identified this through split-half consistency testing, where I ask different questions aimed at uncovering the same underlying belief. For example, I might ask how many would support pausing AI development and then inquire how many think AI will ultimately cause human extinction. These questions consistently converge on roughly the same subset of respondents, providing a reliable measure of this belief within my audience.
    - AI Doom Belief: 20% of my audience expects catastrophic AI outcomes.
    - Split-Half Testing: Method used to validate this belief across different questions.
    - Triangulation: Different questions target similar underlying beliefs.
    - Consistent Convergence: Results consistently point to the same audience segment.
    - Audience Insight: Understanding this helps tailor the rest of the presentation.

- **Slide** 5:46
    The slide title is "How AI Could Spell Disaster." The background of the slide is a dark curtain with a black and silver robot image in the background.
    - Although I believe the likelihood of AI leading to disaster is low, it remains a genuine possibility worth exploring. My probability of AI-driven doom is around 30%, primarily due to the absence of key safeguards, such as an international research organization. Additionally, the risk of widespread suffering is significant--not because AI will intentionally cause harm, but because corporate greed and entrenched power structures that could misuse AI. In this section, we'll examine the concerns of AI doomsayers, taking a serious look at potential scenarios where AI could spell disaster for humanity as we'll steelman the concerns of AI doomers, testing for a genuine possibility.
    - Low Likelihood: Personally assess AI-driven disaster as unlikely (1-30%).
    - Key Milestones: The lack of an international research body heightens risk.
    - Risk of Suffering: Corporate greed and power structures are major concerns.
    - Steelman Approach: Exploring the strongest arguments for AI-driven doom.
    - Genuine Possibility: Taking an honest look at how AI could go wrong.

- **Slide** 7:38
    The slide title is "Bioweapons." The background of the slide is a dark curtain with an image of a green barrel, possibly containing a biohazard, in the foreground.
    - The creation of bioweapons is, in my view, the most significant risk posed by AI. As AI systems advance in material science, exemplified by projects like AlphaFold, the ability to design dangerous and bioengineered agents--such as prions--becomes increasingly chemically accessible. Many of these threats might be "contagious," spreading source and latently, under the smoothness of being released. The COVID-19 pandemic demonstrates how biological agents can evolve, become uncontrollable, and require no energy or weight to spread. The Phase 1 incident causing mass extinction, an engineered project could be the "low-bar" method, at least a precursor to human extinction.  At demonstrably lower, or threat of danger, making these capabilities more accessible to state actors or terrorists.
    - Bioweapon Risk: AI advancements make designing deadly agents easier.
    - Material Science: Projects like AlphaFold increase capabilities in bioengineering.
    - DURC Concerns: Dual-use research heightens the risk of misuse and spread autonomously.
    - Pandemic Lessons: Biological agents can evolve uncontrollably and spread easily.
    - Lowered Threshold: AI reduces the barriers for state actors or terrorists to create bioweapons.
