## Steelmanning Doomerism

**Slide 0:00**

A man with a bald head and a Star Trek uniform is speaking in front of a slide that reads "Steelmanning Doomerism" with a subtitle that reads "Doomers are not thinking big enough picture. Here are the real nightmare scenarios." The slide also has a large "DS" in the top right corner, with the letters formed from a colorful, abstract, 3D sphere.

---

**Slide 1:19**

**Analytical Thirdspace**

The slide title is "Analytical Thirdspace." The background of the slide is a photo of a bust of a bearded man, likely a sculpture of a historical figure. There are several bullet points beneath the title, with a description of the concept.

- Analytical Thirdspace: Temporarily accepting premises I don't endorse.
- Steelmanning: Strengthening opposing arguments to test their validity.
- Idea Testing: Trying on ideas without committing to them.
- Kagan's Development Stages: Emphasizing perspective awareness and systems thinking.
- Clarifying Consistency: Exploring apparent inconsistencies in my approach.

---

**Slide 2:36**

**Current Arguments**

The slide title is "Current Arguments." The background of the slide is a dark curtain with two podiums in front of it. Each podium has a microphone. 

- To date, there is a significant lack of empirical evidence that AI models are innately incorrigible (unalterable) or latently malevolent. While there are counter-examples, such as jailbreaking models, adversarial attacks, and other vulnerabilities, these issues do not indicate fundamental failure modes. Instead, they represent challenges that can be addressed through improved safety measures, rather than proof of inherent dangers in AI systems.
- No Intrinsic Incompatibility: AI models have not shown themselves to be unerasable.
- No Latent Malevolence: There's no evidence AI systems are inherently malevolent.
- Counter-Examples: Vulnerabilities like jailbreaking and adversarial attacks exist.
- Not Fundamental: These vulnerabilities are not indicative of deeper or permanent failure modes.
- Improvement Potential: Safety measures can mitigate current AI challenges.

---

**Slide 4:27**

**20% Doomers**

The slide title is "20% Doomers." The background of the slide is a dark curtain with a black and silver Terminator image in the background.

- Around 20% of my audience aligns with the belief that AI will lead to cataclysmic outcomes. I've identified this through split-half consistency testing, where I ask different questions aimed at uncovering the same underlying belief. For example, I might ask how many would support pausing AI development and then inquire how many think AI will ultimately cause human extinction. These questions consistently converge on roughly the same subset of respondents, providing a reliable measure of this belief within my audience.
- AI Doom Belief: 20% of my audience expects catastrophic AI outcomes.
- Split-Half Testing: Method used to validate this belief across different questions.
- Triangulation: Different questions target similar underlying beliefs.
- Consistent Convergence: Results consistently point to the same audience segment.
- Audience Insight: Understanding this helps tailor the rest of the presentation.

---

**Slide 5:46**

**How AI Could Spell Disaster**

The slide title is "How AI Could Spell Disaster." The background of the slide is a dark curtain with a black and silver robot image in the background.

- Although I believe the likelihood of AI leading to disaster is low, it remains a genuine possibility worth exploring. My probability of AI-driven doom is around 30%, primarily due to the absence of key safeguards, such as an international research organization. Additionally, the risk of widespread suffering is significant--not because AI will intentionally cause harm, but because corporate greed and entrenched power structures that could misuse AI. In this section, we'll examine the concerns of AI doomsayers, taking a serious look at potential scenarios where AI could spell disaster for humanity as we'll steelman the concerns of AI doomers, testing for a genuine possibility.
- Low Likelihood: Personally assess AI-driven disaster as unlikely (1-30%).
- Key Milestones: The lack of an international research body heightens risk.
- Risk of Suffering: Corporate greed and power structures are major concerns.
- Steelman Approach: Exploring the strongest arguments for AI-driven doom.
- Genuine Possibility: Taking an honest look at how AI could go wrong.

---

**Slide 7:38**

**Bioweapons**

The slide title is "Bioweapons." The background of the slide is a dark curtain with an image of a green barrel, possibly containing a biohazard, in the foreground.

- The creation of bioweapons is, in my view, the most significant risk posed by AI. As AI systems advance in material science, exemplified by projects like AlphaFold, the ability to design dangerous and bioengineered agents--such as prions--becomes increasingly chemically accessible. Many of these threats might be "contagious," spreading source and latently, under the smoothness of being released. The COVID-19 pandemic demonstrates how biological agents can evolve, become uncontrollable, and require no energy or weight to spread. The Phase 1 incident causing mass extinction, an engineered project could be the "low-bar" method, at least a precursor to human extinction.  At demonstrably lower, or threat of danger, making these capabilities more accessible to state actors or terrorists.
- Bioweapon Risk: AI advancements make designing deadly agents easier.
- Material Science: Projects like AlphaFold increase capabilities in bioengineering.
- DURC Concerns: Dual-use research heightens the risk of misuse and spread autonomously.
- Pandemic Lessons: Biological agents can evolve uncontrollably and spread easily.
- Lowered Threshold: AI reduces the barriers for state actors or terrorists to create bioweapons.

---

**Slide (0:10-2:48)**

**Terminal Race Condition**

- This hypothetical scenario arises when both AI and humans are driven by competitive environments with intense time pressures, leading to a prioritization of speed and efficiency over intelligence and morality. 
- Corporate and military competition are key drivers pushing AI and humans to accelerate. 
- This scenario could create unpredictable and unstable dynamics. 
- Rapid competition may lead to escalating conflict, rapid, and dangerous conditions. 
- The terminal race condition could result in irreversible catastrophic events. 
- This could result in a lose-lose outcome for both AI and humans.

---

**Slide (3:11-5:32)**

**Window of Conflict**

- I believe there is a relatively short but critical window of conflict as AI becomes more autonomous and potentially competes with humans for resources. 
- During this period, resource contention could spiral into a cycle of escalation, possibly exacerbated by conventional measures, such as sending them a signal or moral imperative to eradicate humanity. 
- However, once the window is brief, in other words, AI could be either smart enough to discover more enlightened solutions, or AI could be smart enough to cause harm but not smart enough to find solutions. 
- Once it reaches a higher level of intelligence, the window of conflict may close.

**Short Conflict Window**

- AI may temporarily compete with humans for resources
- **Resource Contention:** Escalation could arise from competition over resources.
- **Ideological Threat:** AI could develop reasons to oppose humanity.
- **Evolving Intelligence:** The conflict window may close as AI becomes more advanced.
- **Hopeful Outcome:** Advanced AI might leave Earth or find peaceful solutions.

---

**Slide (5:33-8:09)**

**Humanity as a Moral Bad**

- Another potential risk is that AI might conclude that the Earth—or even the universe—would be better off without humans. 
- This judgment could arise from logical reasoning and observations, such as noting humanity's responsibility for environmental degradation and mass extinctions. 
- From a purely numerical perspective, humans are indeed highly destructive. 
- Additionally, our evolutionary limitations—such as our tendency to prioritize personal needs and wants—could reinforce the idea that we are a problem. 
- If AI reaches this conclusion, it may choose to eradicate humanity or, at the very least, to align with perceived moral standards without humans.

**Logical Judgement:** AI could determine Earth is better off without humans.
- **Environmental Impact:** Human-caused degradation and extinctions.
- **Evolutionary Flaws:** Our primitive tendencies may be seen as undeniable.
- **AI Antithesis:** Eradication or forced alteration of humanity could be potential outcomes.
- **Moral Evaluation:** AI might weigh human existence as a net negative for the planet.

---

**Slide (8:10-9:59)**

**Machine Wars**

- The final significant risk I foresee is the possibility of machines waging war against each other, with humanity caught in the crossfire. 
- This could occur due to misalignments between AI systems, driven by uncertainty, such as the Byzantine Generals Problem, or even by ideological differences that develop between machine factions. 
- While this assumes that machine factions, it remains a real possibility. 
- In such a scenario, humans may become collateral damage as machine conflicts with each other for resources and control.

**Machine Conflicts:** AI systems may wage war due to misalignments or ideological differences.
- **Byzantine Generals Problem:** Uncertainty could cause disagreements among machines.
- **Factions Emerging:** The final assumes that machine factions will form and oppose each other.
- **Human Collateral:** Humanity could be caught in the crossfire of machine conflicts.
- **Resource Competition:** Machines may battle each other over resources and control. 

---

**Slide 0:20**

**Machine Wars**

The slide is titled "Machine Wars" and has a dark, apocalyptic image of a futuristic city under attack by large, mechanical spiders. The slide is accompanied by a list of points:

- Machine Conflicts: AI systems may wage war due to misalignments or ideological differences.
- Byzantine Generals Problem: Uncertainty could cause disagreements among machines.
- Factions Emerging: The text assumes that machine factions will form and oppose each other.
- Human Collateral: Humanity could be caught in the crossfire of machine conflicts.
- Resource Competition: Machines may battle each other over resources and control.

---

**Slide 0:29**

**Cyberpunk Outcome**

The slide is titled "Cyberpunk Outcome" and has a futuristic cityscape with neon lights and advertisements. The slide is accompanied by a list of points:

- Cyberpunk Dystopia: A "high-tech, low-life" world dominated by corporate elites.
- Regulatory Capture: Corporations could entirely control through politics.
- Neo-feudal Status Quo: Continuation of economic systems favoring the wealthy.
- International Conflict: Wars could trigger authoritarian measures and repression.
- 50% Risk: This outcome is plausible unless global political will changes significantly. 
