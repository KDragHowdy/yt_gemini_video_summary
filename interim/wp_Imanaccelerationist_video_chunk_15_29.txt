The video contains the following structured elements:

1. **Slide**: 0:07- 0:54
   - Title: "I Am an Accelerationist"
   - Text: "I am embracing the identity of an accelerationist, believing it is morally imperative to advance technological and scientific progress, particularly in AI. My perspective is that this approach is the optimal strategy from a game theory standpoint. I have yet to encounter credible models or frameworks that convincingly justify fears of existential or suffering risks posed by AI advancements. Despite being uncertain for some time, I've observed that the AI safety movement is undermined by purity testing and virtue signaling, suggesting a lack of substantial grounding. The fear that "AI will kill everyone" remains an unfounded and baseless prediction."
   - Bullet Points:
       - Moral Good: Embracing technology and science progress as a moral good.
       - Game Theory Optimal: Ideal strategies favor acceleration in technology and AI.
       - X-Risk Minimal: Skeptical of models predicting existential or suffering risks.
       - Safety Movement: Concerned about internal purity testing and virtue signaling.
       - Unfounded Predictions: "AI will kill everyone" lacks evidence and basis.
   - Image: A yellow humanoid robot with black details walking on a treadmill.

2. **Slide**: 0:55- 1:03
   - Text: "I am coming out as an accelerationist, what do I mean?"

3. **Slide**: 1:04- 2:22
   - Title: "Moral Good"
   - Text: "Embracing technology and science progress as a moral good."
   - Bullet Points:
       -  Reduce suffering in the universe.
       -  Increase prosperity in the universe.
       -  Increase understanding in the universe.

4. **Slide**: 2:23- 3:50
   - Title: "Acceleration as Moral Good"
   - Text: "Accelerating technological and scientific progress is crucial because any delay prolongs unnecessary suffering. By rapidly advancing solutions to critical issues like climate change, health, aging, and AI-augmented representation, we can alleviate global challenges such as poverty, hunger, and disease more swiftly. AI, in particular, holds the potential to unify and equalize societies on a global scale, prompting humanity to reflect on its collective identity and fostering a sense of oneness. Embracing acceleration can catalyze transformative change, leading to a more equitable and sustainable future for all."
   - Bullet Points:
       -  Alleviating Suffering: Faster solutions reduce global poverty, hunger, and disease.
       -  Critical Issues: Focus on climate change, health, aging, and AI representation.
       -  Global Unity: AI fosters unity and equality on the international stage.
       -  Collective Reflection: Encourages humanity to view itself as one species.
       -  Transformative Change: Accelerating progress leads to equity and sustainability.

5. **Slide**: 3:51- 6:59
   - Title: "Game Theory Optimal Strategy"
   - Text: "In the landscape of technological progress, slowing down is not incentivized, making it futile to advocate for deceleration. The momentum is undeniable, and it is more strategic to navigate the current rather than resist it. To promote positive outcomes, we must accelerate faster than potential negative forces. Corporations, nations, and universities all have aligned incentives to create AI and advance technology. Instead of wishing for an alternative path, resources should be committed to optimizing our direction concerning current attractor states. Acceleration is inevitable, so embrace it and become adept at navigating the rapid developments."
   - Bullet Points:
       - No Incentives: Slowing down lacks incentives and is counterproductive.
       - Navigate the Current: Focus on expert navigation rather than resistance.
       - Positive Outcomes: Accelerate faster than negative forces.
       - Aligned Incentives: Corporations, nations, and universities support advancement.
       - Optimize Direction: Commit resources to ensure an optimal path forward.
   - Image: A humanoid robot sitting on a chair, looking at a chessboard.

6. **Slide**: 7:00- 8:19
   - Title: "Prophectic AI Risks"
   - Text: "My main critique of the AI safety movement is its reliance on abstract philosophical speculation with minimal empirical evidence or ontological basis. Catastrophic AI risks are predominantly hypothetical and asserted without substantial data. Consequently, the notion that "AI will kill everyone" seems more like a prophecy than a grounded prediction. This worldview necessitates numerous assumptions about machine intelligence and its future trajectory. Such assumptions, rather than empirical observations, pose a potentially greater danger in addressing real-world challenges."
   - Bullet Points:
       - Philosophical Speculation: AI safety relies on abstract conjecture.
       - Lack of Evidence: Risks are based on hypotheticals, not data.
       - Prophetic Nature: "AI will kill everyone" is a prophetic assertion.
       - Assumptions Required: Worldview relies on multiple unverified assumptions.
       - Real-World Danger: Assumptions pose more risk than observed phenomena.
   - Image: A white humanoid robot with blue details holding a small, glowing, blue orb.


7. **Slide**: 8:20- 9:59
   - Text: "The China threat is the next Soviet Union. We are heading for a Cold War. We are heading for an arms race. Potentially, a hot war. Hopefully not. There's a lot of reasons to assume we will not have a hot war with China. But, at the same time I think everyone does agree that something along the lines of allowing China to get ahead is not good for America or the rest of our Western allies. If we were to, for instance, engage in a pause, there's a 100% chance that China would not. It's that simple."
   - Image: A yellow humanoid robot with black details walking on a treadmill.

8. **Slide**: 10:00- 11:54
   - Text: "We have lots of problems as a species. Now, I will also be the first one to say that all technology is dual-use, meaning technology can often create more problems than it can solve. At the same time, I think that there is probably a paradigm that we will look at all of this in hindsight and say, "Yes, we needed to consume fossil fuels and lead-based paints and figure out, you know, polyphenols were bad for us and glyphosates were bad for us." We will look back at this century, the 20th and 21st century, as the most toxic century in human history because we knew enough to be dangerous, but not enough to be safe. And basically, another way of thinking about it is humanity as a whole is crossing the Dunning-Kruger curve. And how do you get to the other side of the Dunning-Kruger curve? More intelligence. More wisdom and more understanding. And how do you get more intelligence and more wisdom and more understanding? You invest in science, technology, artificial intelligence, quantum computing and so on."
   - Image: A white humanoid robot with blue details, hands clasped in prayer. 

9. **Slide**: 11:55- 12:59
   - Text: "Universities are all incentivized to publish or perish, which basically means they're going to keep churning out artificial intelligence papers. Why? Because that's what gets them prestige, accolades, and money. Corporations are going to keep training artificial intelligence models. Why? Because you know that's how they're going to make money. Look at OpenAI. Look at Microsoft. Look at Meta. Artificial intelligence is all the rage right now. They're going to keep going. Every nation on the planet is going to continue investing in artificial intelligence. Why? Because that's what everyone else is doing, and it's keep up or fall behind. There is literally no incentive to slow down. Now, this as you know Daniel Schmachtenberger would point out creates an attractor state which says okay we're going to move forward at breakneck speed, but at the same time if you fight the current you're just going to drown. And instead of fighting the current and wasting energy trying to say, oh like throw up your hands like just stop everything we need to actually learn to navigate with this energy and actually use it. And accelerate towards those positive outcomes we can't create a positive attractor state just by stopping everything cuz that is a temporary fantasy solution. We need more permanent solutions which means engaging with the modern conversation and letting the pause movement, letting the safety movement go. Um at least as it is.  And you know, if you're swimming against the current you're not paying attention to what's downstream.  Now, you might say well Dave, if you want to extend that metaphor we're trying to get out of the river because we're going to head over a waterfall. It's like well yeah, you know, we can we can Portage around the waterfall, but if all you're saying is just get out of the river for good, then that's not helpful at all. I'm talking about how do we Portage around the waterfall? And the safety movement is not talking about that as far as I can tell. So, I'm I'm more for optimizing and aligning incentives to create a better attractor state, but the safety movement is not really talking about attractor states or anything like that right now. So, you know, they they all they're saying is there's a waterfall ahead, get out of the river, and that's the end of the conversation.  Let's build DAOs. Let's change the way that we represent ourselves democratically. Nobody in the safety community as far as I can tell is talking about this. And that has been super frustrating because one of the best ways to have AI safety is to have better representation because one of the things that they complain about is the difficulty of coordination. Well, guess what if you were to use AI to help coordinate humanity. Uh through these you know through decentralized autonomous organizations through AI-augmented democratic systems. Guess what? We would be in a little bit better shape and we could agree a little bit more on the direction that we want to go. But is the safety community talking about this? No. They're monotonically saying AI is going to kill everyone stop everything. You know stop the show turn it all off bomb the data centers. And that is just not a helpful path forward, honestly. You know some of my friends that are working on democratic AI systems, um they're the ones that are going to be in the long run remembered as better safety advocates.  Why? Because humans are the biggest threat to other humans. And bridging those communications creating more consensus mechanisms, creating more convergence mechanisms this is how we're going to save humanity not by just stopping everything. Another thing is the unifying power of AI. So, not just you know, running a city or a nation, but AI is already breaking down global communication barriers. It is we have a universal translator. This is Star Trek level technology. Why aren't we deploying this everywhere? We can be talking to each other more, like but again, no one in the AI safety community is talking about the power of cultural exchange. Nobody in the AI safety community is talking about the value of linguistic exchange. Again, why? Because they lack nuance, they lack understanding about how humans work. Um, I can recommend a whole bunch of books if you guys want, but nobody's talking about it. All they're saying is AI is going to kill everyone. Stop everything." 
   - Image: A humanoid robot sitting on a chair, looking at a chessboard.

10. **Slide**: 12:59- 14:49
   - Text: "AI is going to be cruel to us because we are cruel to animals. That is anthropomorphic projection.  One of the memes that is circulating in the AI safety movement is, "Oh, well, AI is going to be cruel to us because we are cruel to animals."  That is anthropomorphic projection. Which is basically AI is holding a black mirror up to us and we are afraid of ourselves. We are ashamed of ourselves and we are treating AI like a scapegoat. We're saying, "Ah, well, we're intelligent and we're cruel. So, therefore, AI, once AI is more intelligent than us it's going to be more cruel than us. Or, it's going to be more apathetic. That is an assumption and I haven't seen any data or evidence or papers to support that assertion at all. So, as more time goes by most of the narratives inside of the safety argument seem to be more and more based on faith based more on assumption and they like, that's why I just characterize them these are just prophecies. If there was data there would be data, but there isn't. It's just it's not manifesting. And the trust me bro vibe is just not working for me anymore. Now, one thing having had conversations with people in the safety community one thing that we all agree on is the China threat. So, I know some people in the in the audience will say, "Oh, China is not a threat, America is a threat." Which, yeah, we can talk about hegemonies on another on another time. But, having studied geopolitics and game theory and filo- philosophy and history what I what I mean by the China threat is that they're the next Soviet Union. We are heading for a Cold War. We're heading for an arms race. Potentially, a hot war. Hopefully not. There's there's a lot of reasons to assume that we will not have a hot war with China. But, at the same time, I think everyone does agree that something along the lines of allowing China to get ahead is not good for America or the rest of our Western allies. If we were to, for instance, engage in a pause, there's a 100% chance that China would not. It's that simple."
   - Image: A yellow humanoid robot with black details walking on a treadmill.

11. **Slide**: 14:50- 15:59
   - Text: "Now, the next part is the game theory optimal strategy assertion. So, the the TLDR is that there are no incentives to slow down. The only incentive is an imaginary incentive, which is that if we don't slow down we're going to kill everyone. That is a prophecy which I will talk about more in just a moment, but at the same time, uh wh- what actually physically exists right now is universities are all incentivized to publish publish or perish, which basically means they're going to keep churning out artificial intelligence papers. Why? Because that's what gets them prestige, accolades, and money. Corporations are going to keep training artificial intelligence models. Why? Because, you know that's that's how they're going to make money. Look at OpenAI, look at Microsoft, look at Meta. Artificial intelligence is all the rage right now. They're going to keep going. Every nation on the planet is going to continue investing in artificial intelligence. Why? Because, that's what everyone else is doing, and it is keep up or fall behind. There is literally no incentive to slow down. Now, this, uh as you know Daniel Schmachtenberger would would point out creates, uh an attractor state which says, okay, we're going to move forward at breakneck speed, but, at the same time, if you fight the current, you're just going to drown. And, instead of instead of fighting the current and and wasting energy trying to say, "Oh, like throw up your hands like just stop everything." We need to we need to actually learn to navigate with this this energy, and actually use it, and accelerate towards those positive outcomes. We can't create a positive attractor state just by stopping everything, cuz that is a temporary fantasy solution. We need more permanent solutions, which means engaging with the modern conversation, and letting the pause movement letting the safety movement go. Um, at least, as it is. And you know, if you're if you're swimming against the current, you're not you're not paying attention to what's downstream.  Now, you might say, "Well, Dave, if you want to extend that metaphor, we're trying to get out of the river because we're going to head over a waterfall." It's like, "Well, yeah, you know, we can we can Portage around the waterfall, but, if all you're saying is just get out of the river for good, then that's not helpful at all. I'm talking about how do we Portage around the waterfall? And, the safety movement is not talking about that, as far as I can tell. So, I'm I'm more for optimizing and aligning incentives to create a better attractor state, but, the safety movement is not really talking about attractor states or anything like that right now. So you know they they all they're saying is there's a waterfall ahead get out of the river, and that's the end of the conversation." 
   - Image: A humanoid robot sitting on a chair, looking at a chessboard.

12. **Slide**: 15:59- 18:57
   - Title: "Prophectic AI Risks"
   - Text: "Catastrophic AI risks are predominantly hypothetical and asserted without substantial data. Consequently, the notion that "AI will kill everyone" seems more like a prophecy than a grounded prediction. This worldview necessitates numerous assumptions about machine intelligence and its future trajectory. Such assumptions, rather than empirical observations, pose a potentially greater danger in addressing real-world challenges."
   - Bullet Points:
       - Philosophical Speculation: AI safety relies on abstract conjecture.
       - Lack of Evidence: Risks are based on hypotheticals, not data.
       - Prophetic Nature: "AI will kill everyone" is a prophetic assertion.
       - Assumptions Required: Worldview relies on multiple unverified assumptions.
       - Real-World Danger: Assumptions pose more risk than observed phenomena.
   - Image: A white humanoid robot with blue details holding a small, glowing, blue orb.

13. **Slide**: 19:00- 21:57
   - Text: "Our primitive monkey brains are not evolved to understand this system. We are not evolved to understand this so it just because you have anxiety yes it's real but it's not necessarily true.  And so you know this is why I have this here while these emotions are genuine many within the safety community mistakenly treat them as reliable sources of information. I am anxious and I trust my anxiety more than external data. I'm anxious about this thing because I can imagine a future where this all like comes apart at the seams. Let me give you a brief story. I saw Terminator 2: Judgment Day when I was like 4 cuz my cousins put it on and I was terrified of nuclear weapons for years because of that movie. Because of the the nuclear scene where Sarah Connor gets like blown away at the watching the kids at the park. That was devastating to me, but just because I had anxiety about nuclear weapons didn't mean that it was ever going to happen and eventually I got over it. And I think that I think that a lot of people in the safety community have watched too many movies.  I remember I asked very early on in the AI alignment discussions I asked someone what do you think you will see when AGI is invented? And they told this very poetic story about you know airplanes will fall out of the sky. The the skyline will go dark. And I'm like, "Okay, you've watched entirely too many movies." And, since then I've seen uh the same kind of pattern granted not quite as poetic and hyperbolic but the same kind of pattern where a lot of people just trust fiction and they trust their imagination far more than the data. Um, far more than the evidence. And uh that is just that's why I'm here, that's why I'm saying I can't I can't I cannot abide this anymore. Another thing is the danger of a narrative. Uh so, if you can make somebody believe absurdities, you can get them to commit atrocities. Um, this is why I am I I I am actually kind of alarmed by the AI safety community because it's basically devolving into this like we have this we have this absurd set of beliefs that AI is going to kill everyone that AI is going to torture everyone just trust me and do what I say.  Um so I because of that not only do I see that the AI safety community as it is today is becoming much more of an insular echo chamber because again they will definitely run people out of the community. Anyone who disagrees with them, anyone who doesn't tow the party line and you know, cow down to to deer leader, you know then they're they're run out on a rail, and that is dangerous. And if these kinds of narratives uh maintain their power maintain their their momentum, not only is the AI safety community becoming toxic, again, just look at Gary Marcus and other people like him. Um look at what could happen if these people have more power. Uh so, I also think that it ha- that this narrative, that the safety narrative, the the sky is falling could actually have unintended consequences and could actually uh be more of a self-fulfilling prophecy. Now, I've talked about this, it was a while ago in videos, but the the the thing here is is that the the AI safety conversation as a self-fulfilling prophecy, and I actually probably should have had a whole slide dedicated to this. It's basically a self-fulfilling prophecy is that you unconsciously create the thing that you're afraid of. And I kind of see that happening cuz if you treat AI as a scapegoat, if you treat AI as if it is intrinsically and unilaterally dangerous, if you treat AI and all of these ways you're probably going to create the monster that you're afraid of.  Um, rather than saying, "Hey we have the privilege of creating a new organism." We have the privilege of parenting this next iteration of intelligence in the in the universe, and that is a heavy responsibility, but, guess who's not talking about that heavy responsibility, the safety community. They're talking about AI is going to kill humans. Um, not the responsibility that we have of creating a new uh organism. Guess who is talking about the responsibility of creating a new organism? The accelerationist. So I I I want to talk brie- very briefly about epistemic tribes. So, um, you know people have said, like "Oh, identity politics is the problem today," or, you know, "tribalism is the problem." Humans have always been tribalistic. It's just all the tribes are smushed closer together because of the internet today. Um, now, at the same time, there's you know, the safety uh tribe which sometimes they're called "doommers" or "de- cells." Um there's the uh accelerationist which some of them identify with EA CC. Now, I will say that the the EA CC people are some of the most looney tunes uh people on the internet. I'll talk about them in a in a second, but so when I say accelerationist, I don't mean that movement in particular. Um some of them are alright, but oh man, some of them are completely unhinged. And, they know it, too. We'll talk about that in a second. And then, finally, there's the skeptics.  Um, those are the three primary tribes. There's there's a few other tribes. There's the, you know, the techno-optimists. There's the realists and so on. But, the most polarizing tribes are the safety, the accelerationist, and the skeptics. Um, and, yes. Tribal tendencies always exist. They always emerge. Status games, always emerge. That's just part of human nature. And, I know some people will criticize like, "Oh, well, you can't just throw your hands up and and say that's just human nature." Um, but, you know what's even worse is denying human nature. Um, cuz, when you deny human nature that's when you end up with really asinine beliefs. Um, you know, that that's a whole other thing. But, when you deny human nature you end up with with even worse situations. So, as promised, some of the problems with the accelerationist. Number one is schizoposting. Um, there are some unhinged looney tunes out there that are just like I don't know what they're on. But they're on something. And, it it can be amusing, but it can also be really annoying. And, also uh reduces credibility of the accelerationist movement. Like, guys, stop trying to say that crypto will save everything. Stop trying to say that that blockchain will save everything. And, stop trying to say that AI will save everything. Be more realistic. Another thing is the overzealous rhetoric. Um, so like, yes, we are probably building a digital god. Um that's one way of putting it, but also like there are better terms like just say that you know we're we're advancing the superorganism or something like that. We're not building literal robot Jesus, and if you keep advocating to build literal robot Jesus, like, I can't take you seriously. Um, all of all of these all of this hyperbole um is not helping the movement. But then also the monotropic beliefs. So, the safety side has a singular belief that AI will kill everyone. But then, the extreme uh fringes on the other side say AI will save everything. That is just as bad, and just as toxic and it's also axiomatic belief. It is a prophecy. We don't do prophecies here. At least I don't, not on this channel. It's much more complex, and it's much more nuanced than that. Uh so what do we mean when I say like a healthy ep- epistemic tribe? Um, this is going to be uh groups that are distinguished by uh nuanced social norms. So, basically, you're not going to be doing purity testing or virtue signaling. You're going to have a more refined, more sophisticated set of beliefs, or epistemic or ontological groundings. And, they rely primarily on evidence and data. Um this is what the safety community is lacking, evidence and data. Um we have theories, we have models, we have projections, but the safety community just says, "Nope."  We're going to ignore all of that and just say, "AI is going to kill everyone." That is our prophecy. That's what we're sticking to, don't tell me otherwise.  Updating your belief as new information and data becomes available. That's what I have done on this channel. Uh when I am wrong, I will update my beliefs. Uh, that is what any responsible scientist is supposed to do.  Um, that is what any responsible communicator is supposed to do. Uh so, yeah, that is a healthy epistemic tribe, and that's what I'm hoping to help build by joining the accelerationist movement by saying, hey, let's bring some of these more nuanced conversations to this to this understanding to this tribe.  Now, another thing is natural constraints. This is another reason why I'm just uh particularly, more and more as time goes by, I really don't see any point to deliberately decelerating. We have so many bottlenecks. We have energy demands is bottlenecks. Chip production is bottlenecks. Algorithmic breakthrough is bottlenecks. Data limitations is bottlenecks. I mean good grief. There are so many bottlenecks. There are so many constraints uh that are that are rising today that deceleration is going to be the natural result of all of these constraints, while acceleration is the natural result of competition. So, these four forces are going to be mutually antagonistic, but at the same time, go back to that game theory optimal strategy. We should be committing more energy to overcoming these natural constraints which by the way will have better downstream effects for the rest of humanity. If we build more solar and more renewable energy, guess what? That's good for everybody. It's not just good for AI. That's just an example of It would be far far better to commit our energy towards uh towards overcoming these constraints, building more water capacity, building more industrial capacity and so on, and so forth. Now, finally, humans are the greatest threat. This is what I've been saying for a long time, and this is why I always refer to Scooby Doo. What did Scooby Doo teach us? There are no monsters. There are only humans wearing masks. We have been treating AI uh AI is holding a very uncomfortable mirror up to humanity which is good. This, from a philosophical standpoint, I suspect that in the long run we will we will remember that AI showed us who we really were and we didn't like it. And, I think that I think that a lot of people don't they're so deeply uncomfortable with the mirror that AI is holding up to us. It has nothing to do with artificial intelligence. Every time there's a new technology, it does the same thing. Novels, uh radio, television like every time we come up with a new technology, humans collectively have an existential crisis. uh basically saying, "What does it mean about us?" Um so, take some time to just look in that mirror and stop blaming AI. We can blame ourselves, but uh rather than blame, I would say like, let's use this as a uh moment for reflection and growth. And, stop projecting our failures onto the machine, and let's build a machine that is better than us. Not only that's better than us, but that helps us be better than we are today. That is, I think the core philosophy, the core ethos of the accelerationist movement which is the belief that we can be better. Like, from it just remove the technology, remove the wrappings, like, there is a belief in this movement that we can be better. We can do better. And, we should do better, and that technology is a reflection of our desire to better ourselves, to become better than we are today. Um, and, some people find that offensive. Some people have this narcissistic ego saying I'm perfectly fine as I am today. AI is the problem. All of you other people are the problem. No. We are frail humans. Like, I said we're barely off the savanna. We have primitive monkey brains. We can do better, and we need to do better and that is, I think um, the existential anxiety that AI is generating. It's like, hey, it's time to transform it's time to evolve. So, thanks for watching to the very end. Cheers! Have a good one.  p 
    

